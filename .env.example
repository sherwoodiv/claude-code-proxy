# Required API Keys (only needed for corresponding providers)
ANTHROPIC_API_KEY="your-anthropic-api-key" # Needed if proxying *to* Anthropic
OPENAI_API_KEY="sk-..."
GEMINI_API_KEY="your-google-ai-studio-key"

# Optional: Provider Preference and Model Mapping
# Controls which provider is preferred for mapping haiku/sonnet.
# Supported values: openai, google, anthropic, ollama, custom
# Defaults to openai if not set.
# Set to "anthropic" for "just an Anthropic proxy" mode (no remapping)
# Set to "ollama" for local Ollama models (no public API fallback)
# Set to "custom" for corporate/private APIs (no public API fallback)
PREFERRED_PROVIDER="openai"
OPENAI_BASE_URL="https://api.openai.com/v1"

# Optional: Specify the exact models to map haiku/sonnet to.
# If PREFERRED_PROVIDER=google, these MUST be valid Gemini model names known to the server.
# Defaults to gemini-2.5-pro and gemini-2.5-flash if PREFERRED_PROVIDER=google.
# Defaults to gpt-4.1 and gpt-4.1-mini if PREFERRED_PROVIDER=openai.
# For ollama/custom providers, set these to your available model names.
# These are IGNORED when PREFERRED_PROVIDER=anthropic (models are not remapped).
# BIG_MODEL="gpt-4.1"
# SMALL_MODEL="gpt-4.1-mini"

# Example Google mapping:
# PREFERRED_PROVIDER="google"
# BIG_MODEL="gemini-2.5-pro"
# SMALL_MODEL="gemini-2.5-flash"

# Example Google with vertex AI auth via ADC:
# PREFERRED_PROVIDER="google"
# USE_VERTEX_AUTH=true
# BIG_MODEL="gemini-2.5-pro"
# SMALL_MODEL="gemini-2.5-flash"

# Example "just an Anthropic proxy" mode:
# PREFERRED_PROVIDER="anthropic"
# (BIG_MODEL and SMALL_MODEL are ignored in this mode)

# ============================================
# OLLAMA CONFIGURATION
# ============================================
# Use local Ollama models without falling back to public APIs.
# When PREFERRED_PROVIDER="ollama", all requests are routed to Ollama.

# Example Ollama configuration:
# PREFERRED_PROVIDER="ollama"
# OLLAMA_BASE_URL="http://localhost:11434"
# BIG_MODEL="llama3.3:70b"
# SMALL_MODEL="llama3.2:latest"

# ============================================
# CUSTOM/CORPORATE PROVIDER CONFIGURATION
# ============================================
# Use a custom OpenAI-compatible API (corporate gateway, Azure, etc.)
# without falling back to public APIs.
# When PREFERRED_PROVIDER="custom", all requests use CUSTOM_BASE_URL.

# Example corporate GPT configuration:
# PREFERRED_PROVIDER="custom"
# CUSTOM_BASE_URL="https://your-corporate-api.example.com/v1"
# CUSTOM_API_KEY="your-corporate-api-key"
# BIG_MODEL="gpt-4-corporate"
# SMALL_MODEL="gpt-3.5-corporate"

# ============================================
# CUSTOM OPENAI-COMPATIBLE ENDPOINT
# ============================================
# If using PREFERRED_PROVIDER="openai" with a custom OPENAI_BASE_URL
# that is NOT api.openai.com, the proxy will NOT fall back to public APIs.
# This is useful for Azure OpenAI or other OpenAI-compatible services.

# Example Azure OpenAI configuration:
# PREFERRED_PROVIDER="openai"
# OPENAI_BASE_URL="https://your-resource.openai.azure.com/openai/deployments/your-deployment"
# OPENAI_API_KEY="your-azure-api-key"
# BIG_MODEL="gpt-4"
# SMALL_MODEL="gpt-35-turbo"
